{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train loaded successfully. Rows: 40008\n",
      "✅ valid loaded successfully. Rows: 2717\n",
      "✅ test loaded successfully. Rows: 5007\n",
      "✅ Merged data saved to: C:\\Users\\flopi\\Retrochem\\RetroChem\\Data\\combined_data.csv\n",
      "🧾 Columns in combined_df: ['RxnSmilesClean', 'PseudoHash', 'RetroTemplate', 'TemplateHash']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pathlib\n",
    "\n",
    "base_dir = pathlib.Path().resolve().parent\n",
    "sys.path.append(str(base_dir))\n",
    "\n",
    "# Import functions\n",
    "from RetroChem.Package_functions.Model_training_functions import prepare_fingerprints_for_training\n",
    "\n",
    "#Path to the data directory\n",
    "data_dir = base_dir / \"RetroChem\" / \"Data\"\n",
    "\n",
    "paths = {\n",
    "    \"train\": data_dir / \"reaction_templates_50k_train.csv\",\n",
    "    \"valid\": data_dir / \"reaction_templates_50k_valid.csv\",\n",
    "    \"test\":  data_dir / \"reaction_templates_50k_test.csv\"\n",
    "}\n",
    "\n",
    "# Required columns in input data\n",
    "required_cols = ['RxnSmilesClean', 'PseudoHash', 'RetroTemplate', 'TemplateHash']\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Load and validate files\n",
    "for name, path in paths.items() :\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        missing_cols = set(required_cols) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            print(f\"⚠️  Missing columns in {name}: {missing_cols}\")\n",
    "            continue\n",
    "\n",
    "        dfs.append(df[required_cols])\n",
    "        print(f\"✅ {name} loaded successfully. Rows: {len(df)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {name}: {e}\")\n",
    "\n",
    "# Step 7: Merge and save combined data\n",
    "if dfs:\n",
    "    combined_df = pd.concat(dfs, axis=0).drop_duplicates(subset=['PseudoHash'])\n",
    "    combined_path = data_dir / \"combined_data.csv\"\n",
    "    combined_df.to_csv(combined_path, index=False, sep=\"\\t\")\n",
    "    print(f\"✅ Merged data saved to: {combined_path}\")\n",
    "else:\n",
    "    print(\"❌ No valid files to merge.\")\n",
    "\n",
    "# Step 8: Load the merged file and check columns\n",
    "combined_df = pd.read_csv(data_dir / \"combined_data.csv\", sep=\"\\t\")\n",
    "combined_df.columns = combined_df.columns.str.strip()\n",
    "print(f\"🧾 Columns in combined_df: {combined_df.columns.tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Step 1: Loading the dataset...\n",
      "🔬 Step 3: Transforming data...\n",
      "Start of data processing.\n",
      "Index 0 - SMILES: CC(C)(C)OC(=O)[NH:1][C@H:2]([CH2:3][c:4]1[cH:5][nH:6][c:7]2[cH:8][cH:9][cH:10][cH:11][c:12]12)[c:13]1[n:14][c:15](-[c:16]2[cH:17][cH:18][c:19]([F:20])[cH:21][cH:22]2)[cH:23][nH:24]1>>[NH2:1][C@H:2]([CH2:3][c:4]1[cH:5][nH:6][c:7]2[cH:8][cH:9][cH:10][cH:11][c:12]12)[c:13]1[n:14][c:15](-[c:16]2[cH:17][cH:18][c:19]([F:20])[cH:21][cH:22]2)[cH:23][nH:24]1 | Target: b883d4425d6daa76b98d429b998b1a6ca9aa33cb964e8a98b8b7708b2a9601a3\n",
      "Index 1 - SMILES: CS(=O)(=O)O[CH2:1][C:2]1([NH:3][C:4]([O:5][C:6]([CH3:7])([CH3:8])[CH3:9])=[O:10])[CH2:11][O:12][C:13]([CH3:14])([CH3:15])[O:16][CH2:17]1.[CH3:18][O:19][c:20]1[cH:21][cH:22][c:23]2[c:24]([C:25](=[O:26])[c:27]3[cH:28][c:29]([O:30][CH3:31])[c:32]([O:33][CH3:34])[c:35]([O:36][CH3:37])[cH:38]3)[cH:39][nH:40][c:41]2[cH:42]1>>[CH2:1]([C:2]1([NH:3][C:4]([O:5][C:6]([CH3:7])([CH3:8])[CH3:9])=[O:10])[CH2:11][O:12][C:13]([CH3:14])([CH3:15])[O:16][CH2:17]1)[n:40]1[cH:39][c:24]([C:25](=[O:26])[c:27]2[cH:28][c:29]([O:30][CH3:31])[c:32]([O:33][CH3:34])[c:35]([O:36][CH3:37])[cH:38]2)[c:23]2[cH:22][cH:21][c:20]([O:19][CH3:18])[cH:42][c:41]21 | Target: 90b716e5d44e304fe19eea92b6caea8e8a4eea48f97af91293ae77a89201d2eb\n",
      "Index 2 - SMILES: Br[CH2:1][c:2]1[c:3]([CH3:4])[o:5][c:6](=[O:7])[o:8]1.[CH3:9][C:10](=[O:11])[NH:12][CH2:13][CH2:14][CH2:15][S:16](=[O:17])(=[O:18])[O:19][CH2:20][C:21]([CH3:22])([CH3:23])[C@@H:24]([O:25][Si:26]([CH3:27])([CH3:28])[C:29]([CH3:30])([CH3:31])[CH3:32])[C:33](=[O:34])[OH:35]>>[CH2:1]([c:2]1[c:3]([CH3:4])[o:5][c:6](=[O:7])[o:8]1)[O:35][C:33]([C@@H:24]([C:21]([CH2:20][O:19][S:16]([CH2:15][CH2:14][CH2:13][NH:12][C:10]([CH3:9])=[O:11])(=[O:17])=[O:18])([CH3:22])[CH3:23])[O:25][Si:26]([CH3:27])([CH3:28])[C:29]([CH3:30])([CH3:31])[CH3:32])=[O:34] | Target: e9c1226ab8633ace2fa350eaef368a1d2a6d33595435df3ccee09842ecf230d9\n",
      "Index 3 - SMILES: Cl[c:1]1[o:2][cH:3][c:4]([C:5]([NH:6][CH:7]([CH3:8])[c:9]2[cH:10][c:11]([F:12])[c:13]([NH:14][S:15]([CH3:16])(=[O:17])=[O:18])[c:19]([F:20])[cH:21]2)=[O:22])[n:23]1.[CH3:24][CH2:25][c:26]1[cH:27][cH:28][cH:29][c:30]([OH:31])[cH:32]1>>[c:1]1([O:31][c:30]2[cH:29][cH:28][cH:27][c:26]([CH2:25][CH3:24])[cH:32]2)[o:2][cH:3][c:4]([C:5]([NH:6][CH:7]([CH3:8])[c:9]2[cH:10][c:11]([F:12])[c:13]([NH:14][S:15]([CH3:16])(=[O:17])=[O:18])[c:19]([F:20])[cH:21]2)=[O:22])[n:23]1 | Target: 75a147c7086d7da2df1a2541b8b9335955a926c7809df075b48fd73ed88c29d3\n",
      "Index 4 - SMILES: Cl[CH2:1][CH2:2][N:3]1[CH2:4][CH2:5][CH2:6][CH2:7][CH2:8]1.[CH3:9][O:10][c:11]1[cH:12][c:13]([O:14][CH3:15])[c:16]2[c:17]([cH:18]1)[o:19][c:20](=[O:21])[c:22]1[c:23]2[CH2:24][CH2:25][NH:26][CH2:27]1>>[CH2:1]([CH2:2][N:3]1[CH2:4][CH2:5][CH2:6][CH2:7][CH2:8]1)[N:26]1[CH2:25][CH2:24][c:23]2[c:16]3[c:13]([O:14][CH3:15])[cH:12][c:11]([O:10][CH3:9])[cH:18][c:17]3[o:19][c:20](=[O:21])[c:22]2[CH2:27]1 | Target: afa33c1ecaf9bc583bf82f11c0be777f8f00f59aac8c5f18952f4b616ad6c840\n",
      "Fingerprint preparation finished. Total examples: 128853\n",
      "✅ Step 4: Data transformed into X and y\n",
      "🔍 Checking for missing values in y...\n",
      "✂️ Step 5: Splitting data - 70% train, 15% validation, 15% test\n",
      "🧹 Step 6: Stratifying the temporary set...\n",
      "📊 Dataset sizes: train=97740, val=15556, test=15557\n",
      "✅ Step 7: Data normalized\n",
      "🚀 Starting memory-safe Grid Search and training...\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time= 9.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flopi\\anaconda3\\envs\\env_projet\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=constant, solver=adam; total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flopi\\anaconda3\\envs\\env_projet\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128,), learning_rate=adaptive, solver=adam; total time= 1.6min\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Hide RDKit warnings for more readable output\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# 1. Load the dataset\n",
    "print(\"📁 Step 1: Loading the dataset...\")\n",
    "# Define the path to the data file\n",
    "combined_file_path = os.path.join(data_dir, \"combined_data.csv\")\n",
    "\n",
    "# Read the CSV file with tab separator\n",
    "combined_df = pd.read_csv(combined_file_path, sep=\"\\t\")\n",
    "\n",
    "\n",
    "# 2. Relevant columns\n",
    "cols = ['RxnSmilesClean', 'PseudoHash', 'RetroTemplate', 'TemplateHash']\n",
    "\n",
    "# 3. Transform data into X and y\n",
    "print(\"🔬 Step 3: Transforming data...\")\n",
    "X, y = prepare_fingerprints_for_training(combined_df)\n",
    "print(\"✅ Step 4: Data transformed into X and y\")\n",
    "\n",
    "# 4. Encode y (if categorical labels are strings)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # Convert string labels to integers if necessary\n",
    "\n",
    "# Check and remove missing values in y\n",
    "print(\"🔍 Checking for missing values in y...\")\n",
    "if np.any(np.isnan(y)):\n",
    "    print(\"⚠️ Missing values found in y, removing corresponding rows...\")\n",
    "    mask_valid = ~np.isnan(y)\n",
    "    X = X[mask_valid]\n",
    "    y = y[mask_valid]\n",
    "\n",
    "# 5. Split 70/15/15 (train/validation/test)\n",
    "print(\"✂️ Step 5: Splitting data - 70% train, 15% validation, 15% test\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_idx, temp_idx in sss.split(X, y):\n",
    "    X_train, X_temp = X[train_idx], X[temp_idx]\n",
    "    y_train, y_temp = y[train_idx], y[temp_idx]\n",
    "\n",
    "# 6. Stratification only on frequent classes\n",
    "# Identify rare classes with fewer than 2 samples\n",
    "rare_classes = [class_ for class_, count in pd.Series(y_temp).value_counts().items() if count == 1]\n",
    "\n",
    "# Select frequent classes (with at least 2 samples)\n",
    "frequent_classes = [class_ for class_ in pd.Series(y_temp).value_counts().index if class_ not in rare_classes]\n",
    "\n",
    "# Convert y_temp to pandas.Series for .isin()\n",
    "y_temp_series = pd.Series(y_temp)\n",
    "\n",
    "# Filter X_temp and y_temp to retain only frequent classes\n",
    "mask_frequent = y_temp_series.isin(frequent_classes)\n",
    "X_temp_filtered = X_temp[mask_frequent]\n",
    "y_temp_filtered = y_temp[mask_frequent]\n",
    "\n",
    "# 7. Stratify the filtered temporary set\n",
    "print(\"🧹 Step 6: Stratifying the temporary set...\")\n",
    "sss_val_test = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for val_idx, test_idx in sss_val_test.split(X_temp_filtered, y_temp_filtered):\n",
    "    X_val, X_test = X_temp_filtered[val_idx], X_temp_filtered[test_idx]\n",
    "    y_val, y_test = y_temp_filtered[val_idx], y_temp_filtered[test_idx]\n",
    "\n",
    "# 8. Include rare classes in training without stratification\n",
    "X_train = np.vstack([X_train, X_temp[~mask_frequent]])  # Add rare classes to training set\n",
    "y_train = np.hstack([y_train, y_temp[~mask_frequent]])\n",
    "\n",
    "print(f\"📊 Dataset sizes: train={len(y_train)}, val={len(y_val)}, test={len(y_test)}\")\n",
    "\n",
    "# 9. Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"✅ Step 7: Data normalized\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 10 Define base model\n",
    "base_model = MLPClassifier(\n",
    "    max_iter=50,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 11 Define parameter grid inspired by AIZynthFinder settings\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(1024,), (512, 512), (512, 256)],\n",
    "    'alpha': [1e-5, 1e-4, 1e-3],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['adam'],\n",
    "    'n_iter_no_change': [10]\n",
    "}\n",
    "\n",
    "# 12 Setup GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"🚀 Step 8: Starting Grid Search Training...\")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 13 Best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"✅ Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# 14 Evaluate best model\n",
    "acc_val = accuracy_score(y_val, best_model.predict(X_val_scaled))\n",
    "acc_test = accuracy_score(y_test, best_model.predict(X_test_scaled))\n",
    "print(f\"📈 Validation accuracy: {acc_val:.4f}\")\n",
    "print(f\"📉 Test accuracy: {acc_test:.4f}\")\n",
    "\n",
    "# 15 Plot loss curve\n",
    "plt.plot(best_model.loss_curve_)\n",
    "plt.title(\"📉 Loss curve of best model\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 16 Save best model\n",
    "joblib.dump(best_model, \"best_mlp_classifier_model.pkl\")\n",
    "joblib.dump(scaler, \"new_scaler.pkl\")\n",
    "joblib.dump(label_encoder,\"new_label_encoder.pkl\")\n",
    "\n",
    "print(\"💾 Best model, scaler, and encoder saved successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
