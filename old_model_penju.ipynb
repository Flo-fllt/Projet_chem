{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8241cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# --- Load your dataset (already done before) ---_train.csv\"\n",
    "file_path = r'C:\\Users\\flopi\\projet_reaction_chimie\\uspto50\\uspto50\\reaction_templates_50k_train.csv'\n",
    "df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "colonnes_necessaires = ['RxnSmilesClean', 'PseudoHash', 'RetroTemplate', 'TemplateHash']\n",
    "df = df[colonnes_necessaires]\n",
    "df_unique = df.drop_duplicates(subset=['PseudoHash'], keep='first')\n",
    "\n",
    "# --- 1. Remove atom mapping ---\n",
    "\n",
    "def remove_atom_mapping(smiles):\n",
    "    \"\"\"Remove :number mapping from SMILES.\"\"\"\n",
    "    return re.sub(r\":\\d+\", \"\", smiles)\n",
    "\n",
    "# --- 2. Split SMILES into reactants and products ---\n",
    "\n",
    "def split_rxn_smiles(rxn_smiles):\n",
    "    try:\n",
    "        parts = rxn_smiles.split(\">>\")\n",
    "        if len(parts) != 2:\n",
    "            return [], []\n",
    "        reactants_raw, products_raw = parts\n",
    "        reactants = reactants_raw.split(\".\")\n",
    "        products = products_raw.split(\".\")\n",
    "        return reactants, products\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur dans split_rxn_smiles: {rxn_smiles} -> {e}\")\n",
    "        return [], []\n",
    "\n",
    "# --- 3. Convert SMILES to fingerprints (after cleaning) ---\n",
    "\n",
    "def smiles_to_fingerprints(smiles):\n",
    "    try:\n",
    "        reactants_smiles, products_smiles = split_rxn_smiles(smiles)\n",
    "\n",
    "        def mols_to_fps(smiles_list):\n",
    "            fps = []\n",
    "            n_bits = 2048\n",
    "            for s in smiles_list:\n",
    "                cleaned_s = remove_atom_mapping(s)\n",
    "                mol = Chem.MolFromSmiles(cleaned_s)\n",
    "                if mol is None:\n",
    "                    print(f\"Molécule invalide : {s}\")\n",
    "                    continue  # <-- SKIP invalid molecules\n",
    "                fp = AllChem.GetMorganFingerprint(mol, radius=3)\n",
    "                arr = np.zeros((n_bits,), dtype=int)\n",
    "                if isinstance(fp, Chem.DataStructs.UIntSparseIntVect):\n",
    "                    on_bits = list(fp.GetNonzeroElements().keys())\n",
    "                    for bit in on_bits:\n",
    "                        arr[bit % n_bits] = 1\n",
    "                fps.append(arr)\n",
    "            return fps\n",
    "\n",
    "        reactants_fps = mols_to_fps(reactants_smiles)\n",
    "        products_fps = mols_to_fps(products_smiles)\n",
    "\n",
    "        return reactants_fps, products_fps\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur parsing SMILES: {smiles} -> {e}\")\n",
    "        return [], []\n",
    "\n",
    "# --- 4. Prepare training data X, y ---\n",
    "\n",
    "def prepare_fingerprints_for_training(df):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    print(\"Début du traitement des données.\")\n",
    "    \n",
    "    for idx, (smiles, target) in enumerate(zip(df['RxnSmilesClean'], df['TemplateHash'])):\n",
    "        # Affichage des premières lignes pour vérifier les données\n",
    "        if idx < 5:  # Afficher seulement les 5 premières lignes pour déboguer\n",
    "            print(f\"Index {idx} - SMILES: {smiles} | Target: {target}\")\n",
    "        \n",
    "        reactants_fps, products_fps = smiles_to_fingerprints(smiles)\n",
    "        \n",
    "        if reactants_fps and products_fps:\n",
    "            X.extend(reactants_fps)\n",
    "            y.extend([target] * len(reactants_fps))\n",
    "            X.extend(products_fps)\n",
    "            y.extend([target] * len(products_fps))\n",
    "        else:\n",
    "            print(f\"Skipping reaction at index {idx}: {smiles}\")\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Fingerprint preparation finished. Total examples: {X.shape[0]}\")\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(f\"Mismatch: X {X.shape[0]} vs y {y.shape[0]}\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer X et y\n",
    "X, y = prepare_fingerprints_for_training(df_unique)\n",
    "print(f\"Préparation terminée : X shape = {X.shape}, y shape = {y.shape}\")\n",
    "\n",
    "# --- 3. Entraîner le modèle ---\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Assuming you already have X and y ready\n",
    "# If not, you should prepare them first like before\n",
    "\n",
    "# 1. Scale your data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Define the model\n",
    "model = MLPClassifier(hidden_layer_sizes=(64, 64),\n",
    "                      max_iter=1,  # Only 1 epoch per call\n",
    "                      warm_start=True,  # Keep training across multiple partial_fit\n",
    "                      random_state=42)\n",
    "\n",
    "# 3. Prepare for training\n",
    "n_epochs = 20\n",
    "classes = np.unique(y)  # Needed for partial_fit\n",
    "losses = []  # To track the loss over epochs\n",
    "\n",
    "# 4. Training loop with progress bar and loss tracking\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training Progress\"):\n",
    "    model.partial_fit(X_scaled, y, classes=classes)\n",
    "    losses.append(model.loss_)\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# 5. Save the model and scaler\n",
    "joblib.dump(model, 'mlp_classifier_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Model saved as 'mlp_classifier_model.pkl'\")\n",
    "print(\"Scaler saved as 'scaler.pkl'\")\n",
    "\n",
    "# 6. Plot the loss curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(losses, marker='o')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
